{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Research_Question1.ipynb","provenance":[{"file_id":"1yXle5TlCYR_DqiTICqikc9GU8kHrQO6C","timestamp":1574472667040}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"k81QS1b6WUFW","colab_type":"code","colab":{}},"source":["## https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n","def download_data(a,d):  \n","  import requests\n","  def download_file_from_google_drive(id, destination):\n","      URL = \"https://docs.google.com/uc?export=download\"\n","\n","      session = requests.Session()\n","\n","      response = session.get(URL, params = { 'id' : id }, stream = True)\n","      token = get_confirm_token(response)\n","\n","      if token:\n","          params = { 'id' : id, 'confirm' : token }\n","          response = session.get(URL, params = params, stream = True)\n","\n","      save_response_content(response, destination)    \n","\n","  def get_confirm_token(response):\n","      for key, value in response.cookies.items():\n","          if key.startswith('download_warning'):\n","              return value\n","\n","      return None\n","\n","  def save_response_content(response, destination):\n","      CHUNK_SIZE = 32768\n","\n","      with open(destination, \"wb\") as f:\n","          for chunk in response.iter_content(CHUNK_SIZE):\n","              if chunk: # filter out keep-alive new chunks\n","                  f.write(chunk)\n","  download_file_from_google_drive(a,d)\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LlAT0tmEKYVX","colab_type":"code","colab":{}},"source":["def generate_dataset():\n","  from nltk.tokenize import sent_tokenize,word_tokenize\n","  from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","  import nltk\n","  from nltk.data import load\n","  import numpy as np\n","  from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","  ##https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#TfidfTransformer-to-Compute-Inverse-Document-Frequency-IDF\n","  def sort_coo(coo_matrix):\n","    tuples = zip(coo_matrix.col, coo_matrix.data)\n","    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n","\n","  def extract_topn_from_vector(feature_names, sorted_items, topn):\n","    sorted_items = sorted_items[:topn]\n","    score_vals = [round(s, 3) for i,s in sorted_items]\n","    feature_vals = [feature_names[i] for i,s in sorted_items]\n","\n","    results= {}\n","    for idx in range(len(feature_vals)):\n","      results[feature_vals[idx]]=score_vals[idx]\n","\n","    return results\n","\n","  users,rest,date,review,rating,label=[],[],[],[],[],[]\n","  with open('metadata', 'r', encoding=\"utf-8\") as f:\n","    for l in f:\n","      a=l[:-1].split('\\t')\n","      rating.append(float(a[2]))\n","      label.append(int(a[3]))\n","\n","  with open('reviewContent', 'r', encoding=\"utf-8\") as f:\n","    for l in f:\n","      a=l[:-1].split('\\t')\n","      users.append(int(a[0]))\n","      rest.append(int(a[1]))\n","      date.append(str(a[2]))\n","      review.append(a[3]) \n","\n","  ## Features\n","  tagdict = load('help/tagsets/upenn_tagset.pickle')\n","  sid = SentimentIntensityAnalyzer()\n","  POS,length,avg_word_len,num_sent,avg_sent_len,positive_percent,neg_percent,bi,uni={},[],[],[],[],[],[],{},{}\n","  import pandas as pd\n","  data=pd.DataFrame()\n","  for i in POS:\n","    data[i]=POS[i]\n","\n","  for i in tagdict.keys():\n","    POS['POS_'+i]=[]\n","  \n","  rmv_stop_words=[',','.','/','\"','?',\"'\",'(',')','-','[',']','#','!','@','<','>','+',':',';']\n","\n","  #BIGRAM\n","  vectorizer =TfidfVectorizer(stop_words=rmv_stop_words,ngram_range=(2,2))\n","  X = vectorizer.fit_transform(review)\n","  feature_names=vectorizer.get_feature_names()\n","  sorted_items=sort_coo(X.tocoo())\n","  keywords=extract_topn_from_vector(feature_names,sorted_items,100)          \n","\n","  #unigram\n","  vectorizer_2 =TfidfVectorizer(stop_words=rmv_stop_words,ngram_range=(1,1))\n","  Y = vectorizer_2.fit_transform(review)\n","  feature_names_2=vectorizer_2.get_feature_names()\n","  sorted_items_2=sort_coo(Y.tocoo())\n","  keywords_2=extract_topn_from_vector(feature_names_2,sorted_items_2,100)\n","\n","  for j in keywords:\n","    bi['bigram_'+j]=[0]*len(review)\n","\n","  for j in keywords_2:\n","    uni['unigram_'+j]=[0]*len(review)\n","\n","  dummy_index=0\n","  for i in review:\n","    length.append(len(i))\n","    \n","    ad=[len(z) for z in i.split(\" \")]\n","    avg_word_len.append(sum(ad)/len(ad))\n","    \n","    a= i.split(\" \")\n","    positive_percent.append(len([s for s in a if sid.polarity_scores(s)['compound']>0])/len(i))\n","    neg_percent.append(len([s for s in a if sid.polarity_scores(s)['compound']<0])/len(i))\n","    \n","    num_sent.append(len(sent_tokenize(i)))\n","    \n","    avg_sent_len.append(sum(len(word_tokenize(z)) for z in sent_tokenize(i))/len(sent_tokenize(i)))\n","    \n","    tags = nltk.pos_tag(nltk.word_tokenize(i))\n","    t=[]\n","    for j in tags:\n","      t.append(j[1])\n","    for j in tagdict.keys():\n","      if j in np.unique(np.array(t)):\n","        POS['POS_'+j].append(t.count(j)/len(t))\n","      else:\n","        POS['POS_'+j].append(0)\n","\n","    for j in keywords:\n","      if j in i:\n","        bi['bigram_'+j][dummy_index]=1\n","\n","    for j in keywords_2:\n","      if j in i:\n","        uni['unigram_'+j][dummy_index]=1\n","    dummy_index+=1\n","\n","  #Dataset generation\n","  import pandas as pd\n","  data=pd.DataFrame()\n","  data['rating']=rating\n","  data['length of the review']=length\n","  data['average word length']=avg_word_len\n","  data['number of sentences']=num_sent\n","  data['average sentence length']=avg_sent_len\n","  for i in POS:\n","    data[i]=POS[i]\n","  data['positive %']=positive_percent\n","  data['negative %']=neg_percent\n","  for i in uni:\n","    data[i]=uni[i]\n","  for i in bi:\n","    data[i]=bi[i]\n","  data['label']=label\n","\n","  data.to_csv('First data.csv',index=False)\n","\n","  from google.colab import files as f\n","  f.download('First data.csv')\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3iNj7Jc6KjjN","colab_type":"code","colab":{}},"source":["# The implementation of all the algorithms have been referenced from scikit-learn documentation.\n","def lr(X_train,y_train,X_test,y_test):\n","  from sklearn.linear_model import LogisticRegression\n","  from sklearn.metrics import accuracy_score\n","  LR = LogisticRegression(solver='lbfgs',max_iter=1000)\n","  LR.fit(X_train,y_train)\n","\n","  \n","  print('\\n#####################Logistic Regression#####################')\n","  from sklearn.metrics import classification_report, confusion_matrix\n","  print('==== Training:')\n","  y_pred=LR.predict(X_train)\n","  print(classification_report(y_train,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_train,y_pred))\n","\n","  print('\\n==== Testing:')\n","  y_pred=LR.predict(X_test)\n","  print(classification_report(y_test,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_test,y_pred))  \n","\n","  from sklearn.metrics import accuracy_score\n","  return accuracy_score(y_test,y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQIoFTyfMxkW","colab_type":"code","colab":{}},"source":["def mnb(X_train,y_train,X_test,y_test):\n","  from sklearn.naive_bayes import MultinomialNB\n","  from sklearn.metrics import accuracy_score\n","  mnb=MultinomialNB()\n","  mnb.fit(X_train,y_train)\n","  # y_pred=mnb.predict(X_test)\n","\n","  print('\\n#####################Multinomial Naive Bayes#####################')\n","  from sklearn.metrics import classification_report, confusion_matrix\n","  print('==== Training:')\n","  y_pred=mnb.predict(X_train)\n","  print(classification_report(y_train,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_train,y_pred)) \n","\n","  print('\\n==== Testing:')\n","  y_pred=mnb.predict(X_test)\n","  print(classification_report(y_test,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_test,y_pred))  \n","\n","  from sklearn.metrics import accuracy_score\n","  return accuracy_score(y_test,y_pred)  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rXJq1PBrGud1","colab_type":"code","colab":{}},"source":["def linear_svm(X_train,y_train,X_test,y_test):\n","  from sklearn import svm\n","  clf = svm.SVC(gamma='auto',kernel='linear',cache_size=7000)\n","  clf.fit(X_train,y_train)\n","  y_pred=clf.predict(X_test)\n","  test_score=clf.score( X_test,y_test )\n","  \n","  print('\\n#####################SVM#####################')\n","  from sklearn.metrics import classification_report, confusion_matrix\n","  print(classification_report(y_test,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_test,y_pred))    \n","\n","  from sklearn.metrics import accuracy_score\n","  return accuracy_score(y_test,y_pred)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ulfnBuR7TV-k","colab_type":"code","colab":{}},"source":["def rbf_svm(X_train,y_train,X_test,y_test):\n","  from sklearn import svm\n","  clf = svm.SVC(gamma='auto',kernel='rbf',cache_size=7000)\n","  clf.fit(X_train,y_train)\n","  y_pred=clf.predict(X_test)\n","  test_score=clf.score( X_test,y_test )\n","  \n","  print('\\n#####################SVM#####################')\n","  from sklearn.metrics import classification_report, confusion_matrix\n","  print(classification_report(y_test,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_test,y_pred))    \n","\n","  from sklearn.metrics import accuracy_score\n","  return accuracy_score(y_test,y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0HarfBmTar5","colab_type":"code","colab":{}},"source":["def dtree(X_train,y_train,X_test,y_test):\n","  from sklearn.tree import DecisionTreeClassifier\n","  dt=DecisionTreeClassifier()\n","  dt.fit(X_train,y_train)\n","  \n","  print('\\n#####################Decision Tree#####################')\n","  from sklearn.metrics import classification_report, confusion_matrix\n","  print('==== Training:')\n","  y_pred=dt.predict(X_train)\n","  print(classification_report(y_train,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_train,y_pred)) \n","\n","  print('\\n==== Testing:')\n","  y_pred=dt.predict(X_test)\n","  print(classification_report(y_test,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_test,y_pred))  \n","\n","  from sklearn.metrics import accuracy_score\n","  return accuracy_score(y_test,y_pred)  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0CPbb1gTVdy","colab_type":"code","colab":{}},"source":["def rf(X_train,y_train,X_test,y_test):\n","  from sklearn.ensemble import RandomForestClassifier\n","  clf = RandomForestClassifier(n_estimators=30, max_depth=5,random_state=0)\n","  clf.fit(X_train,y_train)\n","  \n","\n","  print('\\n#####################Random Forest#####################')\n","  from sklearn.metrics import classification_report, confusion_matrix\n","  print('==== Training:')\n","  y_pred=clf.predict(X_train)\n","  print(classification_report(y_train,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_train,y_pred)) \n","\n","  print('\\n==== Testing:')\n","  y_pred=clf.predict(X_test)\n","  print(classification_report(y_test,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_test,y_pred))  \n","\n","  from sklearn.metrics import accuracy_score\n","  return accuracy_score(y_test,y_pred)  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WSS3GrlUTfjR","colab_type":"code","colab":{}},"source":["#The implementation of below algorithm has been referenced from keras documentation\n","def nn(X_train,y_train,X_test,y_test):\n","  from tensorflow.python.util import deprecation\n","  deprecation._PRINT_DEPRECATION_WARNINGS = False\n","  import tensorflow as tf\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","  from keras import Sequential\n","  from keras.layers import Dense,Dropout\n","  num_hidden_layers=3\n","  classifier = Sequential()\n","  \n","  classifier.add(Dense(128, activation='relu', kernel_initializer='random_normal', input_dim=X_train.shape[1]))\n","  for i in range(num_hidden_layers-1):\n","    classifier.add(Dense(128, activation='relu', kernel_initializer='random_normal'))\n","    classifier.add(Dropout(0.2))\n","\n","  classifier.add(Dense(1, activation='softmax', kernel_initializer='random_normal'))\n","\n","  classifier.compile(optimizer ='adam',loss='binary_crossentropy',metrics =['accuracy'])\n","  classifier.fit(X_train,y_train, epochs=20,verbose=0)\n","  # y_pred=classifier.predict(X_test)\n","  # y_pred =(y_pred>0.5)\n","\n","  print('\\n#####################Neural Network#####################')\n","  from sklearn.metrics import classification_report, confusion_matrix\n","  print('==== Training:')\n","  y_pred=classifier.predict(X_train)\n","  print(classification_report(y_train,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_train,y_pred)) \n","\n","  print('\\n==== Testing:')\n","  y_pred=classifier.predict(X_test)\n","  print(classification_report(y_test,y_pred))\n","  print('Confusion Matrix:\\n\\n',confusion_matrix(y_test,y_pred))  \n","\n","  from sklearn.metrics import accuracy_score\n","  return accuracy_score(y_test,y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVJqAg_IZILb","colab_type":"code","colab":{}},"source":["def pca(X1):\n","  acc=[]\n","  from copy import deepcopy as dc\n","  for i in range(1,X1.shape[1]+1):\n","    X=dc(X1)\n","    from sklearn.decomposition import PCA\n","    nf = i\n","    pca = PCA(n_components=nf)\n","    pca.fit(X)\n","    X = pca.fit_transform(X)\n","\n","    from sklearn.model_selection import train_test_split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","    \n","    acc.append(lr(X_train, y_train,X_test,  y_test))\n","  \n","  print(\"Max Acc:\",max(acc) ,\"for:\",np.argmax(acc)+1)  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ysE3wfyEKqOl","colab_type":"code","outputId":"8bd94bfe-6d61-4056-9435-d68832f366f7","executionInfo":{"status":"ok","timestamp":1575827761975,"user_tz":300,"elapsed":656936,"user":{"displayName":"Sumeet Sarode","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBYXR1SNFh1F7gaIrl6An7nqf6HztwepUzilqlbzA=s64","userId":"17089771790885864806"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#Main function\n","def main():\n","  # generate_dataset()\n","\n","  #download data\n","  a='1-pNvy5k7PTivT7qql-wN7PzEZvHa076c'\n","  d='data.csv'\n","  download_data(a,d)\n","\n","  import pandas as pd\n","  import numpy as np\n","\n","  data = pd.read_csv('data.csv')\n","  X1=data.iloc[:,:-1]\n","  y=data.iloc[:,-1]\n","  \n","  # pca(X1)\n","\n","  from sklearn.model_selection import train_test_split\n","  X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2)\n","\n","  a1=mnb(X_train,y_train,X_test,y_test)\n","\n","  # print(\"Before Scaling\")\n","  # a1=mnb(X_train,y_train,X_test,y_test)\n","  # a2=dtree(X_train, y_train,X_test,y_test)\n","  # a3=lr(X_train, y_train,X_test,y_test)  \n","  # a4=rf(X_train,y_train,X_test,y_test)\n","  # a5=nn(X_train,y_train,X_test,y_test)\n","\n","  # print(\"\\nMinMax Scaler\")\n","  # from sklearn.preprocessing import MinMaxtScaler\n","  # scaler = MinMaxScaler()\n","  # scaler.fit(X1)\n","  # X3 = scaler.transform(X1)\n","  # X_train, X_test, y_train, y_test = train_test_split(X3, y, test_size=0.2)\n","  # a2=dtree(X_train, y_train,X_test,y_test)\n","  # a3=lr(X_train, y_train,X_test,y_test)  \n","  # a4=rf(X_train,y_train,X_test,y_test)\n","  # a5=nn(X_train,y_train,X_test,y_test)\n","\n","  # print(\"\\nStandard Scaler\")\n","  # from sklearn.preprocessing import StandardScaler\n","  # scaler = StandardScaler()\n","  # scaler.fit(X1)\n","  # X3 = scaler.transform(X1)\n","  # X_train, X_test, y_train, y_test = train_test_split(X3, y, test_size=0.2)\n","  # a2=dtree(X_train, y_train,X_test,y_test)\n","  # a3=lr(X_train, y_train,X_test,y_test)  \n","  # a4=rf(X_train,y_train,X_test,y_test)\n","  # a5=nn(X_train,y_train,X_test,y_test)\n","\n","  # print(\"\\nRobust Scaler\")\n","  # from sklearn.preprocessing import RobustScaler\n","  # scaler = RobustScaler()\n","  # scaler.fit(X1)\n","  # X3 = scaler.transform(X1)\n","  # X_train, X_test, y_train, y_test = train_test_split(X3, y, test_size=0.2)\n","  # a2=dtree(X_train, y_train,X_test,y_test)\n","  # a3=lr(X_train, y_train,X_test,y_test)  \n","  # a4=rf(X_train,y_train,X_test,y_test)\n","  # a5=nn(X_train,y_train,X_test,y_test)\n","\n","  from sklearn.preprocessing import Normalizer\n","  scaler=Normalizer()\n","  scaler.fit(X1)\n","  X2 = scaler.transform(X1)\n","  X_train, X_test, y_train, y_test = train_test_split(X2,y,test_size=0.2)\n","  a2=dtree(X_train, y_train,X_test,y_test)\n","  a3=lr(X_train, y_train,X_test,y_test)  \n","  a4=rf(X_train,y_train,X_test,y_test)\n","  a5=nn(X_train,y_train,X_test,y_test)\n","\n","  print(\"\\n\\nTesting Accuracies:\\nMultinomial NB:\",a1,\"\\nDecision Tree:\",a2,\"\\nLogistic Regression:\",a3,\"\\nRandom Forest:\",a4,\"\\nNeural Network:\",a5)\n"," \n","if __name__ == \"__main__\":\n","    main()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["\n","#####################Multinomial Naive Bayes#####################\n","==== Training:\n","              precision    recall  f1-score   support\n","\n","          -1       0.18      0.57      0.27     64260\n","           1       0.90      0.60      0.72    422618\n","\n","    accuracy                           0.59    486878\n","   macro avg       0.54      0.58      0.49    486878\n","weighted avg       0.81      0.59      0.66    486878\n","\n","Confusion Matrix:\n","\n"," [[ 36522  27738]\n"," [170993 251625]]\n","\n","==== Testing:\n","              precision    recall  f1-score   support\n","\n","          -1       0.18      0.56      0.27     16206\n","           1       0.90      0.60      0.72    105514\n","\n","    accuracy                           0.59    121720\n","   macro avg       0.54      0.58      0.49    121720\n","weighted avg       0.80      0.59      0.66    121720\n","\n","Confusion Matrix:\n","\n"," [[ 9138  7068]\n"," [42546 62968]]\n","\n","#####################Decision Tree#####################\n","==== Training:\n","              precision    recall  f1-score   support\n","\n","          -1       0.99      1.00      1.00     64434\n","           1       1.00      1.00      1.00    422444\n","\n","    accuracy                           1.00    486878\n","   macro avg       1.00      1.00      1.00    486878\n","weighted avg       1.00      1.00      1.00    486878\n","\n","Confusion Matrix:\n","\n"," [[ 64245    189]\n"," [   327 422117]]\n","\n","==== Testing:\n","              precision    recall  f1-score   support\n","\n","          -1       0.19      0.21      0.20     16032\n","           1       0.88      0.86      0.87    105688\n","\n","    accuracy                           0.78    121720\n","   macro avg       0.53      0.54      0.53    121720\n","weighted avg       0.79      0.78      0.78    121720\n","\n","Confusion Matrix:\n","\n"," [[ 3387 12645]\n"," [14656 91032]]\n","\n","#####################Logistic Regression#####################\n","==== Training:\n","              precision    recall  f1-score   support\n","\n","          -1       0.33      0.00      0.00     64434\n","           1       0.87      1.00      0.93    422444\n","\n","    accuracy                           0.87    486878\n","   macro avg       0.60      0.50      0.47    486878\n","weighted avg       0.80      0.87      0.81    486878\n","\n","Confusion Matrix:\n","\n"," [[    71  64363]\n"," [   141 422303]]\n","\n","==== Testing:\n","              precision    recall  f1-score   support\n","\n","          -1       0.36      0.00      0.00     16032\n","           1       0.87      1.00      0.93    105688\n","\n","    accuracy                           0.87    121720\n","   macro avg       0.62      0.50      0.47    121720\n","weighted avg       0.80      0.87      0.81    121720\n","\n","Confusion Matrix:\n","\n"," [[    21  16011]\n"," [    37 105651]]\n","\n","#####################Random Forest#####################\n","==== Training:\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","          -1       0.00      0.00      0.00     64434\n","           1       0.87      1.00      0.93    422444\n","\n","    accuracy                           0.87    486878\n","   macro avg       0.43      0.50      0.46    486878\n","weighted avg       0.75      0.87      0.81    486878\n","\n","Confusion Matrix:\n","\n"," [[     0  64434]\n"," [     0 422444]]\n","\n","==== Testing:\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","          -1       0.00      0.00      0.00     16032\n","           1       0.87      1.00      0.93    105688\n","\n","    accuracy                           0.87    121720\n","   macro avg       0.43      0.50      0.46    121720\n","weighted avg       0.75      0.87      0.81    121720\n","\n","Confusion Matrix:\n","\n"," [[     0  16032]\n"," [     0 105688]]\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","#####################Neural Network#####################\n","==== Training:\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","          -1       0.00      0.00      0.00     64434\n","           1       0.87      1.00      0.93    422444\n","\n","    accuracy                           0.87    486878\n","   macro avg       0.43      0.50      0.46    486878\n","weighted avg       0.75      0.87      0.81    486878\n","\n","Confusion Matrix:\n","\n"," [[     0  64434]\n"," [     0 422444]]\n","\n","==== Testing:\n","              precision    recall  f1-score   support\n","\n","          -1       0.00      0.00      0.00     16032\n","           1       0.87      1.00      0.93    105688\n","\n","    accuracy                           0.87    121720\n","   macro avg       0.43      0.50      0.46    121720\n","weighted avg       0.75      0.87      0.81    121720\n","\n","Confusion Matrix:\n","\n"," [[     0  16032]\n"," [     0 105688]]\n","\n","\n","Testing Accuracies:\n","Multinomial NB: 0.5923923759447913 \n","Decision Tree: 0.7757065395990799 \n","Logistic Regression: 0.8681564245810056 \n","Random Forest: 0.8682878738087414 \n","Neural Network: 0.8682878738087414\n"],"name":"stdout"}]}]}